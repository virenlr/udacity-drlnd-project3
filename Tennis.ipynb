{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collaboration and Competition\n",
    "\n",
    "---\n",
    "\n",
    "In this notebook, you will learn how to use the Unity ML-Agents environment for the third project of the [Deep Reinforcement Learning Nanodegree](https://www.udacity.com/course/deep-reinforcement-learning-nanodegree--nd893) program.\n",
    "\n",
    "### 1. Start the Environment\n",
    "\n",
    "We begin by importing the necessary packages.  If the code cell below returns an error, please revisit the project instructions to double-check that you have installed [Unity ML-Agents](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Installation.md) and [NumPy](http://www.numpy.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will start the environment!  **_Before running the code cell below_**, change the `file_name` parameter to match the location of the Unity environment that you downloaded.\n",
    "\n",
    "- **Mac**: `\"path/to/Tennis.app\"`\n",
    "- **Windows** (x86): `\"path/to/Tennis_Windows_x86/Tennis.exe\"`\n",
    "- **Windows** (x86_64): `\"path/to/Tennis_Windows_x86_64/Tennis.exe\"`\n",
    "- **Linux** (x86): `\"path/to/Tennis_Linux/Tennis.x86\"`\n",
    "- **Linux** (x86_64): `\"path/to/Tennis_Linux/Tennis.x86_64\"`\n",
    "- **Linux** (x86, headless): `\"path/to/Tennis_Linux_NoVis/Tennis.x86\"`\n",
    "- **Linux** (x86_64, headless): `\"path/to/Tennis_Linux_NoVis/Tennis.x86_64\"`\n",
    "\n",
    "For instance, if you are using a Mac, then you downloaded `Tennis.app`.  If this file is in the same folder as the notebook, then the line below should appear as follows:\n",
    "```\n",
    "env = UnityEnvironment(file_name=\"Tennis.app\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: TennisBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 8\n",
      "        Number of stacked Vector Observation: 3\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 2\n",
      "        Vector Action descriptions: , \n"
     ]
    }
   ],
   "source": [
    "env = UnityEnvironment(file_name=\"C://Users/viren/Desktop/p3_collab-compet/Tennis_Windows_x86_64/Tennis.exe\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environments contain **_brains_** which are responsible for deciding the actions of their associated agents. Here we check for the first brain available, and set it as the default brain we will be controlling from Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Examine the State and Action Spaces\n",
    "\n",
    "In this environment, two agents control rackets to bounce a ball over a net. If an agent hits the ball over the net, it receives a reward of +0.1.  If an agent lets a ball hit the ground or hits the ball out of bounds, it receives a reward of -0.01.  Thus, the goal of each agent is to keep the ball in play.\n",
    "\n",
    "The observation space consists of 8 variables corresponding to the position and velocity of the ball and racket. Two continuous actions are available, corresponding to movement toward (or away from) the net, and jumping. \n",
    "\n",
    "Run the code cell below to print some information about the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 2\n",
      "Size of each action: 2\n",
      "There are 2 agents. Each observes a state with length: 24\n",
      "The state for the first agent looks like: [ 0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.         -6.65278625 -1.5\n",
      " -0.          0.          6.83172083  6.         -0.          0.        ]\n"
     ]
    }
   ],
   "source": [
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents \n",
    "num_agents = len(env_info.agents)\n",
    "print('Number of agents:', num_agents)\n",
    "\n",
    "# size of each action\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Size of each action:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "print('There are {} agents. Each observes a state with length: {}'.format(states.shape[0], state_size))\n",
    "print('The state for the first agent looks like:', states[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Take Random Actions in the Environment\n",
    "\n",
    "In the next code cell, you will learn how to use the Python API to control the agents and receive feedback from the environment.\n",
    "\n",
    "Once this cell is executed, you will watch the agents' performance, if they select actions at random with each time step.  A window should pop up that allows you to observe the agents.\n",
    "\n",
    "Of course, as part of the project, you'll have to change the code so that the agents are able to use their experiences to gradually choose better actions when interacting with the environment!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(1, 6):                                      # play game for 5 episodes\n",
    "#     env_info = env.reset(train_mode=False)[brain_name]     # reset the environment    \n",
    "#     states = env_info.vector_observations                  # get the current state (for each agent)\n",
    "#     scores = np.zeros(num_agents)                          # initialize the score (for each agent)\n",
    "#     while True:\n",
    "#         actions = np.random.randn(num_agents, action_size) # select an action (for each agent)\n",
    "#         actions = np.clip(actions, -1, 1)                  # all actions between -1 and 1\n",
    "#         env_info = env.step(actions)[brain_name]           # send all actions to tne environment\n",
    "#         next_states = env_info.vector_observations         # get next state (for each agent)\n",
    "#         rewards = env_info.rewards                         # get reward (for each agent)\n",
    "#         dones = env_info.local_done                        # see if episode finished\n",
    "#         scores += env_info.rewards                         # update the score (for each agent)\n",
    "#         states = next_states                               # roll over states to next time step\n",
    "#         if np.any(dones):                                  # exit loop if episode finished\n",
    "#             break\n",
    "#     print('Score (max over agents) from episode {}: {}'.format(i, np.max(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When finished, you can close the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. It's Your Turn!\n",
    "\n",
    "Now it's your turn to train your own agent to solve the environment!  When training the environment, set `train_mode=True`, so that the line for resetting the environment looks like the following:\n",
    "```python\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, we have loaded our Unity environment and tested a random agent to ensure that everything works as expected. Now begins the code that will train and use DDPG agents to solve the problem.\n",
    "\n",
    "Let us start by importing all the remaining files we need to make this happen. Please note that MADDPG, imported in the line **from maddpg import MADDPG**, uses a modified version of the DDPG code used in the Pendulum project present in Udacity's Github repository for this nanodegree. This file (and its dependencies *Actor* and *Critic* from the *model.py* file) are also imported at this stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from collections import deque\n",
    "\n",
    "from maddpg import MADDPG\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now instantiate the agents. The observation space consists of 8 variables corresponding to the position and velocity of the ball and racket. Each agent receives its own, local observation. Two continuous actions are available, corresponding to movement toward (or away from) the net, and jumping.\n",
    "\n",
    "All this information is passed to our agents in the next code cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "agents = MADDPG(state_size, action_size, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Learning Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanation of the Learning Algorithm**\n",
    "\n",
    "**DDPG (Deep Deterministic Policy Gradient):** This is a type of Actor-Critic method and can be seen as an approximate Deep Q-Network instead of an actual Actor-Critic. Here, the Critic is used to approximate the maximizer over the next state's Q-values and not as a learned baseline.\n",
    "\n",
    "One of the limitations of the Deep Q-Learning agent is that it is not straightforward to use in continuous action spaces. In DDPG, we use two neural networks- the \"Actor\" and the \"Critic\". The Actor is used to approximate the optimal policy deterministically- outputting the best-believed action for any given state. This is unlike stochastic policies, where we want the policy to learn a probability distribution over the actions. The Actor essentially learns the arg_max a Q(s, a), which is the best action. The Critic learns to evaluate the optimal action-value function by using the action the Actor believes to be the best. We use the Actor, an approximate maximizer, to calculate a new target value for training the action-value function in the way Deep Q-Networks do.\n",
    "\n",
    "DDPGs use Replay Buffers and make \"soft-updates\" to the target networks. In Deep Q-Networks, we maintain two copies of the network weights, the regular and target networks. The target network is updated, say, every 10,000-time steps, by copying the regular network's weights. In DDPGs, there are 4 networks- a regular and a target for the Actor and the Critic. The target networks are updated with soft-updates, that is, slowly blending the regular network weights with the target's, over a period of time, and not all at once. In practice, this yields faster convergence and can be used with other algorithms that use target networks.\n",
    "\n",
    "**Modifications in MADDPG:** To make DDPG work in a multi-agent scenario, certain changes need to be made. We start by creating a new Python file, `maddpg.py`. Then, we move the ReplayBuffer logic into this file and create a class that handles both agents; we create custom `step`, `act`, and `reset` methods that handle both agents. The logic is as follows: A common replay buffer is shared between the agents. Both agents draw from this buffer, and both add to the buffer as well. Thus, the agents learn from each other's experiences.\n",
    "\n",
    "```\n",
    "class MADDPG:\n",
    "    def __init__(self, state_size, action_size, seed):\n",
    "        super(MADDPG, self).__init__()\n",
    "        \n",
    "        self.maddpg_agents = [DDPGAgent(state_size, action_size, 1*seed),\n",
    "                              DDPGAgent(state_size, action_size, 2*seed)]\n",
    "        \n",
    "        self.memory = ReplayBuffer(BUFFER_SIZE, BATCH_SIZE, seed)\n",
    "\n",
    "    def step(self, time_step, states, actions, rewards, next_states, dones):\n",
    "        \"\"\"Save experience in replay memory, and use random sample from buffer to learn.\"\"\"\n",
    "        \n",
    "        # Save experience / reward for the agents\n",
    "        self.memory.add(states[0], actions[0], rewards[0], next_states[0], dones[0])\n",
    "        self.memory.add(states[1], actions[1], rewards[1], next_states[1], dones[1])\n",
    "\n",
    "        # Learn, if enough samples are available in memory\n",
    "        if len(self.memory) > BATCH_SIZE:\n",
    "            \n",
    "            for agent in self.maddpg_agents:\n",
    "                experiences = self.memory.sample()\n",
    "                agent.step(experiences)\n",
    "                    \n",
    "    def act(self, states):\n",
    "        \"\"\"For each agent, return the action to take\"\"\"\n",
    "        \n",
    "        actions = np.zeros([2, 2]) # 2 agents and action consists of 2 values\n",
    "        \n",
    "        actions[0, :] = self.maddpg_agents[0].act(states[0])\n",
    "        actions[1, :] = self.maddpg_agents[1].act(states[1])\n",
    "        \n",
    "        return actions\n",
    "        \n",
    "    def reset(self):        \n",
    "        for agent in self.maddpg_agents:\n",
    "            agent.reset()\n",
    "```\n",
    "\n",
    "Some modifications have to be made to the ddpg code as well. We use Gaussian noise instead of OUNoise and have the noise decay with each timestep. This allows the training to converge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanation of Hyperparameters used**\n",
    "\n",
    "`BUFFER_SIZE = int(1e5)`\n",
    "\n",
    "The size of the Replay Buffer that holds tuples of experiences that can be selected multiple times.\n",
    "\n",
    "`BATCH_SIZE = 256`\n",
    "\n",
    "The number of entries from the Replay Buffer that are considered a part of each batch.\n",
    "\n",
    "`GAMMA = 0.95`\n",
    "\n",
    "The hyperparameter that prioritizes how much weightage is given to recently received rewards, as compared to previous rewards.\n",
    "\n",
    "`TAU = 0.01`\n",
    "\n",
    "The architecture makes use of two networks (for the Actor and Critic, each)- a fixed network and a target network. This hyperparameter is used for providing soft updates to the target network; i.e., instead of updating the values all at once, the process happens gradually, controlled with this hyperparameter.\n",
    "\n",
    "`LR_ACTOR = 1e-4`\n",
    "\n",
    "The learning rate with which the Actor model learns.\n",
    "\n",
    "`LR_CRITIC = 1e-3`\n",
    "\n",
    "The learning rate with which the Critic model learns.\n",
    "\n",
    "`WEIGHT_DECAY = 0`\n",
    "\n",
    "The L2 weight decay used to enhance the speed of convergence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanation of Model Architecture**\n",
    "\n",
    "There are a total of four networks *per agent* - a regular and target for the Actor, and a regular and target for the Critic. All networks have identical architectures as follows:\n",
    "\n",
    "They consist of an input layer with 8 nodes (for the 8 dimension vector provided as the state), followed by two layers of 512 and 256 neurons respectively. ReLu activations are used to maintain the non-linearity in the models. The output layer consists of two nodes, representing a vector of the continuous action the agent must take. Tanh activations at this layer ensure that the output lies between -1 and +1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanation of Training Process**\n",
    "\n",
    "The agents play the game, adding their experiences to the ReplayBuffer. Simultaneously, a sample is taken from the buffer at every time step from this common buffer to train the agents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100\tAverage Score: 0.012100\n",
      "Episode 200\tAverage Score: 0.025700\n",
      "Episode 300\tAverage Score: 0.071400\n",
      "Episode 400\tAverage Score: 0.423500\n",
      "Episode 409\tAverage score: 0.507500\n",
      "Environment solved in 309 episodes!\tAverage Score: 0.507500\n"
     ]
    }
   ],
   "source": [
    "def maddpg(n_episodes=10000, print_every=100):\n",
    "    \n",
    "    scores_deque = deque(maxlen=print_every)\n",
    "    score_tracker = []\n",
    "    \n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        \n",
    "        # Instead of saying: state = env.reset(), we must first create an env_info object as shown above\n",
    "        \n",
    "        env_info = env.reset(train_mode=True)[brain_name]              # reset the environment\n",
    "        states = env_info.vector_observations                          # get the current state (for each agent)\n",
    "        \n",
    "        agents.reset()\n",
    "        \n",
    "        scores = np.zeros(num_agents)                                  # initialize the score (for each agent)\n",
    "        \n",
    "        t = 0\n",
    "        \n",
    "        while(True):\n",
    "            t = t+1\n",
    "            actions = agents.act(states)                               # select an action (for each agent)\n",
    "            \n",
    "            # Instead of saying: next_state, reward, done, _ = env.step(action), we must do this step as shown above\n",
    "            \n",
    "            env_info = env.step(actions)[brain_name]                   # send all actions to tne environment\n",
    "            next_states = env_info.vector_observations                 # get next state (for each agent)\n",
    "            rewards = env_info.rewards                                 # get the reward (for each agent)\n",
    "            dones = env_info.local_done                                # see if episode has finished\n",
    "            \n",
    "            # Pass in the time step as well so the agents know when they need to update\n",
    "            agents.step(t, states, actions, rewards, next_states, dones)\n",
    "            \n",
    "            states = next_states                                       # roll over states to next time step\n",
    "            scores += rewards                                          # update the score (for each agent)\n",
    "            \n",
    "            if np.any(dones):                                          # exit loop if episode finished\n",
    "                break\n",
    "                \n",
    "        scores_deque.append(max(scores))\n",
    "        score_tracker.append(max(scores))\n",
    "        \n",
    "        print('\\rEpisode {}\\tAverage score: {:.6f}'.format(i_episode, np.mean(scores_deque)), end=\"\")\n",
    "        \n",
    "        if i_episode % print_every == 0:\n",
    "            print('\\rEpisode {}\\tAverage Score: {:.6f}'.format(i_episode, np.mean(scores_deque)))\n",
    "            \n",
    "        if np.mean(scores_deque)>=0.5:\n",
    "            print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.6f}'.format(i_episode-100, np.mean(scores_deque)))\n",
    "            \n",
    "            for index, agent in enumerate(agents.maddpg_agents):\n",
    "                torch.save(agent.actor_local.state_dict(), 'checkpoint_actor_agent_{}.pth'.format(index+1))\n",
    "                torch.save(agent.critic_local.state_dict(), 'checkpoint_critic_agent_{}.pth'.format(index+1))\n",
    "            \n",
    "            break\n",
    "            \n",
    "    return score_tracker\n",
    "\n",
    "scores = maddpg()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Plot of Rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This plot of rewards per episode has been included to illustrate that the agents are capable of receiving a maximum average reward of +0.5 over the last 100 episodes. From the previous code cell, we see that we have solved the environment in **309** episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot the scores\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(len(scores)), scores)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now go ahead and close the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Ideas for Future Work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having gone through Udacity's Knowledge portal, I see there are many things I can experiment with:\n",
    "\n",
    "- Some people have gotten better performance with using a bigger buffer or minibatch size, different learning rates, and different architectures for the DNNs that correspond to the Actor and Critic.\n",
    "\n",
    "- Other algorithms covered in the lecture like A3C, A2C, and GAE.\n",
    "\n",
    "- Adapting the code to work with the Unity Soccer environment."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
